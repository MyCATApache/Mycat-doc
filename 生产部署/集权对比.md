总：集群的选择
By writer:zuoyuezong@126.com
需要注意的地方
一.介绍
项目地址： http://www.percona.com/doc/percona-xtradb-cluster/intro.html
Percona XtraDB Cluster是MySQL高可用性和可扩展性的解决方案.
Percona XtraDB Cluster提供的特性有：
1.同步复制，事务要么在所有节点提交或不提交。
2.多主复制，可以在任意节点进行写操作。
3.在从服务器上并行应用事件，真正意义上的并行复制。
4.节点自动配置。
5.数据一致性，不再是异步复制。
Percona XtraDB Cluster完全兼容MySQL和Percona Server，表现在：
1.数据的兼容性
2.应用程序的兼容性：无需更改应用程序
 
1.集群是有节点组成的，推荐配置至少3个节点，但是也可以运行在2个节点上。
2.每个节点都是普通的mysql/percona服务器，可以将现有的数据库服务器组成集群，反之，也可以将集群拆分成单独的服务器。
3.每个节点都包含完整的数据副本。
优点如下：
1.当执行一个查询时，在本地节点上执行。因为所有数据都在本地，无需远程访问。
2.无需集中管理。可以在任何时间点失去任何节点，但是集群将照常工作。
3.良好的读负载扩展，任意节点都可以查询。
缺点如下：
1.加入新节点，开销大。需要复制完整的数据。
2.不能有效的解决写缩放问题，所有的写操作都将发生在所有节点上。
3.有多少个节点就有多少重复的数据。
架构图如下：
Percona XtraDB Cluster与MySQL Replication区别在于：
分布式系统的CAP理论。
C---一致性，所有节点的数据一致。
A---可用性，一个或多个节点失效，不影响服务请求。
P---分区容忍性，节点间的连接失效，仍然可以处理请求。
任何一个分布式系统，需要满足这三个中的两个。
MySQL Replication: 可用性和分区容忍性
Percona XtraDB Cluster: 一致性和可用性
因此MySQL Replication并不保证数据的一致性，而Percona XtraDB Cluster提供数据一致性。
Percona XtraDB Cluster组件：
Percona XtraDB Cluster基于XtraDB的Percona Server以及包含写复制集补丁。使用Galera 2.x library，事务型应用下的通用的多主同步复制插件。
Galera 2.x新特性有：
1.IST(Incremental State Transfer)增量状态传输。对于WAN特别有用。
2.RSU(Rolling Schema Update)旋转更新架构。不会阻止对表进行操作。
 
二.初始配置
为了使用XtraDB集群，需要在my.cnf文件中配置以下选项：
wsrep_provider -- a path to Galera library.
wsrep_cluster_address -- cluster connection URL.
binlog_format=ROW
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2
innodb_locks_unsafe_for_binlog=1
# 额外的参数有：
wsrep_slave_threads #指定线程数量
wsrep_sst_method
4.安装XtraBackup SST方法
为了使用Percona XtraBackup的State Transfer method(节点间数据的快照副本拷贝)。可以使用支持Galera信息的脚本的正式的xtrabackup包，可以从innobackupex源码包中得到innobackupex脚本。同时在my.cnf文件中制定：
wsrep_sst_method=xtrabackup
 

三.局限性 仅Innodb
1.目前的复制仅仅支持InnoDB存储引擎。任何写入其他引擎的表，包括mysql.*表将不会复制。但是DDL语句会被复制的，因此创建用户将会被复制，但是insert into mysql.user...将不会被复制的。
2.DELETE操作不支持没有主键的表。没有主键的表在不同的节点顺序将不同，如果执行SELECT...LIMIT... 将出现不同的结果集。
3.在多主环境下LOCK/UNLOCK TABLES不支持。以及锁函数GET_LOCK(), RELEASE_LOCK()...
4.查询日志不能保存在表中。如果开启查询日志，只能保存到文件中。
5.允许最大的事务大小由wsrep_max_ws_rows和wsrep_max_ws_size定义。任何大型操作将被拒绝。如大型的LOAD DATA操作。
6.由于集群是乐观的并发控制，事务commit可能在该阶段中止。如果有两个事务向在集群中不同的节点向同一行写入并提交，失败的节点将中止。对于集群级别的中止，集群返回死锁错误代码(Error: 1213 SQLSTATE: 40001 (ER_LOCK_DEADLOCK)).
7.XA事务不支持，由于在提交上可能回滚。
8.整个集群的写入吞吐量是由最弱的节点限制，如果有一个节点变得缓慢，那么整个集群将是缓慢的。为了稳定的高性能要求，所有的节点应使用统一的硬件。
9.集群节点建议最少3个。
10.如果DDL语句有问题将破坏集群。
四．集群内部机制写入数据id自增递增比如集群中三台可能是3 6 9这样类似递增
五．刚做好这个集群每一个节点的日志都要保留时间长点最好是每个节点都做日志备份，数据备份防止哪一个节点退出集群但是数据库没有挂掉然后haproxy还是在分发这样的数据不一致解决起来没有比对十分麻烦使用toad for mysql进行比对那时，总而言之言而总之这个集群坑还是不少的


一.Mariadb集群+haproxy+keepalived
作者：zuoyuezong@126.com

注意sysbench5.0 版本与 sysbench4.2版本测试的结果将会大不相同这里是同环境作比较
1.安装mariadb集群
1.服务器环境如下：

 
1颗双核 xeon e5-2680的CPU 
[root@zyz_dba_test01 ~]# head -4 /proc/meminfo 
MemTotal:        1922464 kB
MemFree:           79904 kB
Buffers:          159296 kB
Cached:          1366644 kB
2G不到的内存

2.系统环境如下
Red Hat Enterprise Linux Server release 6.4 (Santiago)
3.集群版本
mariadb-galera-10.0.17-linux-x86_64.tar.gz
Haproxy
Haproxy是一个反向代理负载均衡解决方案，支持4层和7层模式，提供后端服务器健康检查，非常稳定。淘宝前期也使用Haproxy作为CDN系统负载均衡器

haproxy版本：1.4.25


4.ip地址
10.21.3.106 node1
10.21.3.107 node2
10.21.3.108 node3  
10.21.3.109 haproxy
配置到每台的/etc/hosts文件中

5.安装
1.node1 node2 node3 执行以下操作：
  ln -sf /usr/lib64/libssl.so.10 /usr/lib64/libssl.so.6
  ln -sf /usr/lib64/libcrypto.so.10 /usr/lib64/libcrypto.so.6
    
2.文件传达一个目录如/usr/local/sc 在3台上解压
```
tar -xf  /usr/local/src/mariadb-galera-10.0.17-linux-x86_64.tar.gz
mv   /usr/local/src/mariadb-galera-10.0.17-linux-x86_64 /usr/local/mysql
cd mysql/scripts
mkdir /data/db
useradd mariadb
chown mariadb:mariadb /data/db
./mysql_install_db --datadir=/data/db/ --basedir=/usr/local/mysql/ --user=mariadb
```
以下分别的node1 node2 node3的配置文件
```
 cat  > /usr/local/mysql/my1.cnf  <<OO
[client]
#password       = your_password
port            = 3306
socket          = /tmp/mysql.sock
[mysqld]
port            = 3306
user=mariadb
socket          = /tmp/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
log-bin=mysql-bin
binlog_format = ROW
server-id       = 1
innodb_autoinc_lock_mode = 2
wsrep_provider = /usr/local/mysql/lib/libgalera_smm.so
wsrep_cluster_name = "my_mariadb_cluster"
wsrep_cluster_address="gcomm://"
wsrep_cluster_name='example_cluster'
wsrep_node_name = "cluster_node1"

wsrep_node_address = 10.21.3.106:4406
wsrep_sst_auth=tt:123
wsrep_node_name='node1'
wsrep_sst_method=rsync

[mysqldump]
quick
max_allowed_packet = 16M
[mysql]
no-auto-rehash
[myisamchk]
key_buffer_size = 8M
sort_buffer_size = 8M
[mysqlhotcopy]
interactive-timeout
OO
```
Node2配置文件:
```
cat > /etc/my.cnf << PP
[client]
#password       = your_password
port            = 3306
socket          = /tmp/mysql.sock


[mysqld]
port            = 3306
user=mariadb
socket          = /tmp/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
datadir=/data/db
log-bin=mysql-bin
binlog_format = ROW
server-id       = 2

innodb_autoinc_lock_mode = 2
wsrep_provider = /usr/local/mysql/lib/libgalera_smm.so
wsrep_cluster_name = "my_mariadb_cluster"
wsrep_cluster_address="gcomm://10.21.3.106,10.21.3.108"
wsrep_cluster_name='example_cluster'
wsrep_node_name = "cluster_node2"
wsrep_node_address = 10.21.3.107:4406
wsrep_sst_auth=tt:123
wsrep_node_name='node2'
wsrep_sst_method=rsync
[mysqldump]
quick
max_allowed_packet = 16M
[mysql]
no-auto-rehash
[myisamchk]
key_buffer_size = 8M
sort_buffer_size = 8M
[mysqlhotcopy]
interactive-timeout
PP
```
Node3配置文件：
```
cat > /etc/my.cnf << HH
[client]
#password       = your_password
port            = 3306
socket          = /tmp/mysql.sock


[mysqld]
port            = 3306
user=mariadb
socket          = /tmp/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
datadir=/data/db
 max_connections=1000 
log-bin=mysql-bin

binlog_format = ROW

server-id       = 3

innodb_autoinc_lock_mode = 2

wsrep_provider = /usr/local/mysql/lib/libgalera_smm.so

wsrep_cluster_name = "my_mariadb_cluster"

wsrep_cluster_address="gcomm://10.21.3.106,10.21.3.107"
wsrep_cluster_name='example_cluster'

wsrep_node_name = "cluster_node3"

wsrep_node_address = 10.21.3.108:4406
wsrep_sst_auth=tt:123
wsrep_node_name='node3'
wsrep_sst_method=rsync

[mysqldump]
quick
max_allowed_packet = 16M

[mysql]
no-auto-rehash

[myisamchk]
key_buffer_size = 8M
sort_buffer_size = 8M

[mysqlhotcopy]
interactive-timeout
HH
```

顺序启动数据库
Node2和node3上（node1不需要）
cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 

Node1: /usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/my1.cnf --wsrep-new-cluster
Node2:service mysqld restart
Node3:serivce mysqld restart
测试在任意一台创建删除数据均会同步
6.压力测试
 
```SQL
CREATE TABLE  test.`sbtest` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `k` int(10) unsigned NOT NULL DEFAULT '0',
  `c` char(120) NOT NULL DEFAULT '',
  `pad` char(60) NOT NULL DEFAULT '',
  PRIMARY KEY (`id`),
  KEY `k` (`k`)
) ENGINE=InnoDB;
create table test.sbtest1  select * from  test.sbtest; create table test.sbtest2  select * from  test.sbtest;create table test.sbtest3  select * from  test.sbtest;create table test.sbtest4  select * from  test.sbtest;create table test.sbtest5  select * from  test.sbtest;create table test.sbtest6  select * from  test.sbtest;create table test.sbtest7 select * from  test.sbtest;create table test.sbtest8  select * from  test.sbtest;create table test.sbtest9  select * from  test.sbtest;create table test.sbtest10  select * from  test.sbtest;

create database sbtest;create table sbtest.sbtest  select * from  test.sbtest;create table sbtest.sbtest1  select * from  test.sbtest;create table sbtest.sbtest2  select * from  test.sbtest;create table sbtest.sbtest3  select * from  test.sbtest;create table sbtest.sbtest4  select * from  test.sbtest;create table sbtest.sbtest5  select * from  test.sbtest;create table sbtest.sbtest6  select * from  test.sbtest;create table sbtest.sbtest7  select * from  test.sbtest;create table sbtest.sbtest8  select * from  test.sbtest;create table sbtest.sbtest9  select * from  test.sbtest;create table sbtest.sbtest10  select * from  test.sbtest;
```
安装sysbench
[root@zyz_dba_test02~]#wget http://www.percona.com/redir/downloads/Percona-XtraDB-Cluster/5.5.37-25.10/RPM/rhel6/x86_64/Percona-XtraDB-Cluster-shared-55-5.5.37-25.10.756.el6.x86_64.rpm
[root@zyz_dba_test02~]#yum -y install Percona-XtraDB-Cluster-shared-55-5.5.37-25.10.756.el6.x86_64.rpm 
[root@zyz_dba_test02 ~]# rpm -ivh sysbench-0.5-2.el6_.x86_64.rpm 
Preparing...                ########################################### [100%]
   1:sysbench               ########################################### [100%]
至此安装完毕
sysbench --test=/usr/share/doc/sysbench/tests/db/select.lua --mysql-table-engine=innodb --oltp-table-size=1000000 --max-requests=0 --max-time=60 --num-threads=100 --oltp-tables-count=10 --report-interval=1 --mysql-host=10.21.3.108 --mysql-port=3306 --mysql-user=root --mysql-password=123 --mysql-db=test run
测试下并发800的情况下的QPS 读
 
CPU 已经使用load 43.19
 
由上图可以看出基本上在以上服务器环境只读能达到14015.38


 
CPU占用负载0.3的时候也就是正常值的时候平均值不到5000 
```
root@zyz_dba_test03 ~]# sysbench --test=/usr/share/doc/sysbench/tests/db/select.lua --mysql-table-engine=innodb --oltp-table-size=1000000 --max-requests=0 --max-time=60 --num-threads=1 --oltp-tables-count=10 --report-interval=1 --mysql-host=10.21.3.108 --mysql-port=3306 --mysql-user=root --mysql-password=123 --mysql-db=test run
sysbench 0.5:  multi-threaded system evaluation benchmark

Running the test with following options:
Number of threads: 1
Report intermediate results every 1 second(s)
Random number generator seed is 0 and will be ignored


Threads started!

[   1s] threads: 1, tps: 0.00, reads/s: 4603.85, writes/s: 0.00, response time: 0.34ms (95%)
[   2s] threads: 1, tps: 0.00, reads/s: 5050.07, writes/s: 0.00, response time: 0.28ms (95%)
[   3s] threads: 1, tps: 0.00, reads/s: 5080.91, writes/s: 0.00, response time: 0.28ms (95%)
[   4s] threads: 1, tps: 0.00, reads/s: 4807.06, writes/s: 0.00, response time: 0.28ms (95%)
[   5s] threads: 1, tps: 0.00, reads/s: 4794.00, writes/s: 0.00, response time: 0.29ms (95%)
[   6s] threads: 1, tps: 0.00, reads/s: 5018.65, writes/s: 0.00, response time: 0.28ms (95%)
[   7s] threads: 1, tps: 0.00, reads/s: 5119.29, writes/s: 0.00, response time: 0.28ms (95%)
[   8s] threads: 1, tps: 0.00, reads/s: 4835.05, writes/s: 0.00, response time: 0.30ms (95%)
[   9s] threads: 1, tps: 0.00, reads/s: 4900.95, writes/s: 0.00, response time: 0.30ms (95%)
[  10s] threads: 1, tps: 0.00, reads/s: 4981.05, writes/s: 0.00, response time: 0.28ms (95%)
[  11s] threads: 1, tps: 0.00, reads/s: 5090.04, writes/s: 0.00, response time: 0.27ms (95%)
[  12s] threads: 1, tps: 0.00, reads/s: 5091.90, writes/s: 0.00, response time: 0.27ms (95%)
[  13s] threads: 1, tps: 0.00, reads/s: 5085.02, writes/s: 0.00, response time: 0.28ms (95%)
[  14s] threads: 1, tps: 0.00, reads/s: 5111.03, writes/s: 0.00, response time: 0.28ms (95%)
[  15s] threads: 1, tps: 0.00, reads/s: 5062.60, writes/s: 0.00, response time: 0.29ms (95%)
[  16s] threads: 1, tps: 0.00, reads/s: 5044.30, writes/s: 0.00, response time: 0.28ms (95%)
[  17s] threads: 1, tps: 0.00, reads/s: 3402.04, writes/s: 0.00, response time: 0.28ms (95%)
[  18s] threads: 1, tps: 0.00, reads/s: 5106.10, writes/s: 0.00, response time: 0.28ms (95%)
[  19s] threads: 1, tps: 0.00, reads/s: 5096.99, writes/s: 0.00, response time: 0.27ms (95%)
[  20s] threads: 1, tps: 0.00, reads/s: 4992.88, writes/s: 0.00, response time: 0.28ms (95%)
[  21s] threads: 1, tps: 0.00, reads/s: 5110.09, writes/s: 0.00, response time: 0.28ms (95%)
[  22s] threads: 1, tps: 0.00, reads/s: 5148.97, writes/s: 0.00, response time: 0.28ms (95%)
[  23s] threads: 1, tps: 0.00, reads/s: 5077.03, writes/s: 0.00, response time: 0.28ms (95%)
[  24s] threads: 1, tps: 0.00, reads/s: 5065.69, writes/s: 0.00, response time: 0.28ms (95%)
[  25s] threads: 1, tps: 0.00, reads/s: 5071.30, writes/s: 0.00, response time: 0.28ms (95%)
[  26s] threads: 1, tps: 0.00, reads/s: 5095.95, writes/s: 0.00, response time: 0.28ms (95%)
[  27s] threads: 1, tps: 0.00, reads/s: 5141.07, writes/s: 0.00, response time: 0.27ms (95%)
[  28s] threads: 1, tps: 0.00, reads/s: 5095.01, writes/s: 0.00, response time: 0.28ms (95%)
[  29s] threads: 1, tps: 0.00, reads/s: 5130.99, writes/s: 0.00, response time: 0.28ms (95%)
[  30s] threads: 1, tps: 0.00, reads/s: 5040.05, writes/s: 0.00, response time: 0.29ms (95%)
[  31s] threads: 1, tps: 0.00, reads/s: 4992.92, writes/s: 0.00, response time: 0.29ms (95%)
[  32s] threads: 1, tps: 0.00, reads/s: 4390.00, writes/s: 0.00, response time: 0.28ms (95%)
[  33s] threads: 1, tps: 0.00, reads/s: 4949.77, writes/s: 0.00, response time: 0.28ms (95%)
[  34s] threads: 1, tps: 0.00, reads/s: 4619.22, writes/s: 0.00, response time: 0.28ms (95%)
[  35s] threads: 1, tps: 0.00, reads/s: 4519.03, writes/s: 0.00, response time: 0.28ms (95%)
[  36s] threads: 1, tps: 0.00, reads/s: 5154.03, writes/s: 0.00, response time: 0.28ms (95%)
[  37s] threads: 1, tps: 0.00, reads/s: 4816.98, writes/s: 0.00, response time: 0.28ms (95%)
[  38s] threads: 1, tps: 0.00, reads/s: 5111.99, writes/s: 0.00, response time: 0.28ms (95%)
[  39s] threads: 1, tps: 0.00, reads/s: 5146.03, writes/s: 0.00, response time: 0.27ms (95%)
[  40s] threads: 1, tps: 0.00, reads/s: 5025.99, writes/s: 0.00, response time: 0.27ms (95%)
[  41s] threads: 1, tps: 0.00, reads/s: 5105.00, writes/s: 0.00, response time: 0.27ms (95%)
[  42s] threads: 1, tps: 0.00, reads/s: 5053.86, writes/s: 0.00, response time: 0.28ms (95%)
[  43s] threads: 1, tps: 0.00, reads/s: 4647.15, writes/s: 0.00, response time: 0.29ms (95%)
[  44s] threads: 1, tps: 0.00, reads/s: 5068.91, writes/s: 0.00, response time: 0.29ms (95%)
[  45s] threads: 1, tps: 0.00, reads/s: 5105.05, writes/s: 0.00, response time: 0.28ms (95%)
[  46s] threads: 1, tps: 0.00, reads/s: 5094.96, writes/s: 0.00, response time: 0.27ms (95%)
[  47s] threads: 1, tps: 0.00, reads/s: 5066.08, writes/s: 0.00, response time: 0.27ms (95%)
[  48s] threads: 1, tps: 0.00, reads/s: 4955.88, writes/s: 0.00, response time: 0.27ms (95%)
[  49s] threads: 1, tps: 0.00, reads/s: 4237.05, writes/s: 0.00, response time: 0.28ms (95%)
[  50s] threads: 1, tps: 0.00, reads/s: 4958.96, writes/s: 0.00, response time: 0.29ms (95%)
[  51s] threads: 1, tps: 0.00, reads/s: 4997.98, writes/s: 0.00, response time: 0.29ms (95%)
[  52s] threads: 1, tps: 0.00, reads/s: 4984.12, writes/s: 0.00, response time: 0.29ms (95%)
[  53s] threads: 1, tps: 0.00, reads/s: 4818.93, writes/s: 0.00, response time: 0.28ms (95%)
[  54s] threads: 1, tps: 0.00, reads/s: 5099.00, writes/s: 0.00, response time: 0.28ms (95%)
[  55s] threads: 1, tps: 0.00, reads/s: 5079.00, writes/s: 0.00, response time: 0.28ms (95%)
[  56s] threads: 1, tps: 0.00, reads/s: 5119.94, writes/s: 0.00, response time: 0.27ms (95%)
[  57s] threads: 1, tps: 0.00, reads/s: 4636.10, writes/s: 0.00, response time: 0.29ms (95%)
[  58s] threads: 1, tps: 0.00, reads/s: 5023.99, writes/s: 0.00, response time: 0.28ms (95%)
[  59s] threads: 1, tps: 0.00, reads/s: 5041.95, writes/s: 0.00, response time: 0.29ms (95%)
[  60s] threads: 1, tps: 0.00, reads/s: 4971.98, writes/s: 0.00, response time: 0.29ms (95%)

OLTP test statistics:
    queries performed:
        read:                            297036
        write:                           0
        other:                           0
        total:                           297036
    transactions:                        0      (0.00 per sec.)
    deadlocks:                           0      (0.00 per sec.)
    read/write requests:                 297036 (4950.56 per sec.)
    other operations:                    0      (0.00 per sec.)

General statistics:
    total time:                          60.0005s
    total number of events:              297036
    total time taken by event execution: 59.5268s
    response time:
         min:                                  0.10ms
         avg:                                  0.20ms

         max:                                333.52ms
         approx.  95 percentile:               0.28ms

Threads fairness:
    events (avg/stddev):           297036.0000/0.00
execution time (avg/stddev):   59.5268/0.00

```
2个线程的时候：
``
sysbench --test=/usr/share/doc/sysbench/tests/db/select.lua --mysql-table-engine=innodb --oltp-table-size=1000000 --max-requests=0 --max-time=90 --num-threads=2 --oltp-tables-count=10 --report-interval=1 --mysql-host=10.21.3.108 --mysql-port=3306 --mysql-user=root --mysql-password=123 --mysql-db=test run
 
``
 


后面我又做了mysql5.6.24的压测同一个环境 安装的mysql在3307端口
结果证明还是mysql5.6.24的tps 和qps更加的高一些
 
 
Qps相差还不是太大（使用的是sysbench0.5版本的 0.4版本的话mariadb 测试的tps还是和0.5版本有比较大的区别的因此这里的话是同一环境下mariadb10.0.17集群和mysql5.6.24的单实例的比较回头会安装上mysql5.6.24集群的情况下再做压测比较）
 TPS：
```
 
[root@zyz_dba_test03 ~]#  sysbench --test=/usr/share/doc/sysbench/tests/db/oltp.lua --oltp_tables_count=9 --oltp-table-size=10 --rand-init=on --num-threads=1000  --oltp-read-only=off --report-interval=1 --rand-type=gaussian --max-time=3000 --max-requests=0 --mysql-host=10.21.3.108 --mysql-port=3307 --mysql-user=root --mysql-password=123 run
sysbench 0.5:  multi-threaded system evaluation benchmark

Running the test with following options:
Number of threads: 1000
Report intermediate results every 1 second(s)
Initializing random number generator from timer.

Random number generator seed is 0 and will be ignored


Threads started!

[   1s] threads: 1000, tps: 0.00, reads/s: 11258.02, writes/s: 7.00, response time: 0.00ms (95%)
[   2s] threads: 1000, tps: 217.97, reads/s: 5323.38, writes/s: 927.89, response time: 1956.38ms (95%)
[   3s] threads: 1000, tps: 380.05, reads/s: 5429.68, writes/s: 1501.19, response time: 2941.13ms (95%)
[   4s] threads: 1000, tps: 624.99, reads/s: 3677.92, writes/s: 2810.94, response time: 3578.21ms (95%)
[   5s] threads: 1000, tps: 280.01, reads/s: 5856.11, writes/s: 907.02, response time: 2867.24ms (95%)
[   6s] threads: 1000, tps: 230.97, reads/s: 4535.40, writes/s: 1125.85, response time: 2860.39ms (95%)
[   7s] threads: 1000, tps: 358.06, reads/s: 6401.04, writes/s: 1108.18, response time: 3526.11ms (95%)
[   8s] threads: 1000, tps: 293.91, reads/s: 4854.51, writes/s: 1237.62, response time: 4222.37ms (95%)
```
还行在1000个线程的情况下基本能达到300TPS
在100个线程的情况下基本能达到600二mariadb集群呢仅仅能达到60差距甚至不止10倍
 sysbench --test=/usr/share/doc/sysbench/tests/db/oltp.lua --oltp_tables_count=9 --oltp-table-size=10 --rand-init=on --num-threads=100  --oltp-read-only=off --report-interval=1 --rand-type=gaussian --max-time=3000 --max-requests=0 --mysql-host=10.21.3.108 --mysql-port=3307 --mysql-user=root --mysql-password=123 run
结论qps 测试同意环境mysql5.6.24稍强 tps5.6.24强太多
2.Haproxy
Haproxy 简述：
HAProxy是一个开源的、高性能的，基于TCP(第四层)和HTTP(第7层)应用的负载均衡软件。使用HAProxy可以快速可靠地实现基于TCP和HTTP应用的负载均衡解决方案。
作为一个专业的负载均衡软件，他有如下优点：
1.可靠性和稳定性非常好，可以与硬件级的F5负载均衡设备想媲美
2.最高可以同时维护40000-50000个并发连接，单位时间处理的最大请求数达20000个，最大数据处理能力可达10Gbps。
3.支持多达8种负载均衡算法，同时也支持session保持
4.支持虚拟主机功能，使实现web负载均衡更加灵活
5.从HAProxy 1.3版本后开始支持连接拒绝、全透明代理等功能，这些功能是其他负载均衡器所不具备的。
6.有功能强大的监控页面，通过此页面可以实时了解系统的运行状况。
7.拥有功能强大的ACL支持。
HAProxy是借助操作系统的技术特性来实现性能最大化的，因此要想发挥HAProxy的最大性能需要对操作系统性能进行优化。
HAProxy非常适用于那些并发量特别大且需要持久连接或四层和七层处理机制的web系统，例如门户网站或电子商务网站等。
HAProxy也可以用于MySQL数据库（读操作）的负载均衡。
HAProxy的官方文档地址：
http://cbonte.github.io/haproxy-dconv/

1.安装配置


由于这几天haproxy 官网关掉了 所以就在51CTO上下载了个包
```
yum -y install gcc 
[root@zyz_dba_test04 ~]# tar -xf haproxy-1.4.25 -C /usr/local/
[root@zyz_dba_test04 ~]# cd /usr/local/haproxy/
[root@zyz_dba_test04 haproxy]# useradd haproxy
[root@zyz_dba_test04 haproxy]# make TARGET=linux26 PREFIX=/usr/local/haproxy
[root@zyz_dba_test04 haproxy]# make install PREFIX=/usr/local/haproxy 
[root@zyz_dba_test04 haproxy]# mkdir /usr/local/haproxy/logs/ -p && mkdir /usr/local/haproxy/var/run/ -p && mkdir /usr/local/haproxy/var/chroot/ -p && mkdir /usr/local/haproxy/conf/ -p
[root@zyz_dba_test04 haproxy]#  cat  > /usr/local/haproxy/haproxy.cfg  << OO 
global
        log            127.0.0.1        local0
        log            127.0.0.1        local1 notice
        maxconn        94096
        user            haproxy      #所属运行的用户
        group           haproxy      #所属运行的组
        nbproc          1
        pidfile         /usr/local/haproxy/var/run/haproxy1.pid

defaults
        log            global
        option         tcplog
        option         dontlognull
        retries         3
        option          redispatch
        maxconn         94096
        timeout         connect  50000ms
        timeout         client   50000ms
        timeout         server   50000ms

listen  mariadb-galera
        bind 10.21.3.109:3399  #客户端监听端口
        mode tcp
        balance  leastconn  #最少连接的负载均衡算法
        server   db1  10.21.3.106:3306 check
        server   db2  10.21.3.107:3306 check
        server   db3  10.21.3.108:3306 check
OO


```


加入页面监控
```
global
        log 127.0.0.1 local0 info #[err warning info debug]
        maxconn 40960
        user    haproxy
        group   haproxy
        daemon
        nbproc 3
        chroot  /usr/local/haproxy/var/chroot
        pidfile /usr/local/haproxy/var/run/haproxy.pid


defaults
        log            global
        option         tcplog
        option         dontlognull
        retries         3
        option          redispatch
        maxconn         94096
        timeout         connect  50000ms
        timeout         client   50000ms
        timeout         server   50000ms
        timeout check 2000

listen admin_stats
        bind 0.0.0.0:1080
        mode http
        log 127.0.0.1 local0 err #[err warning info debug]
        stats refresh 3s
        stats uri /admin?stats
        stats realm Gemini\ Haproxy
        stats auth admin:admin
        stats auth admin1:admin1


listen  mariadb-galera
        bind 10.1.166.45:3399  #客户端监听端口
        mode tcp
        balance  leastconn  #最少连接的负载均衡算法
        server   db1  10.1.166.45:3306 check
        server   db2  10.1.166.46:3306 check
        server   db3  10.1.166.47:3306 check
        server   db4  10.1.166.48:3306 check
        server   db5  10.1.166.49:3306 check
```
，nbproc进程如果设置大于1则会包警告
六、其它汇总
1、/usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/etc/haproxy.cfg
[WARNING] 217/202150 (2857) : Proxy ’chinaapp.sinaapp.com’: in multi-process mode, stats will be limited to process assigned to the current request.
会提示如上信息，nbproc进程如果设置为1则不会提示，如果想去掉这个提示可以修改编译文件即可。 在源码配置src/cfgparse.c找到如下行
if (nbproc > 1) {
if (curproxy->uri_auth) {
- Warning(“Proxy ‘%s’: in multi-process mode, stats will be limited to process assigned to the current request.\n”,
+ Warning(“Proxy ‘%s’: in multi-process mode, stats will be limited to the process assigned to the current request.\n”,
调整nbproc > 1数值即可。
然而经过我的实验感觉并没有什么卵用 因此还是在listen admin_stats下面
加上靠谱

2.启动测试
``
[root@zyz_dba_test04 ~]# /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg &
``
找集群中一台授权
```
MariaDB [(none)]> grant all privileges on *.* to zuo@'%' identified by '123';
Query OK, 0 rows affected (0.00 sec)
[root@zyz_dba_test04 haproxy]# mysql -h 10.21.3.109  -uzuo -p123  --port 3399
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 14
Server version: 10.0.17-MariaDB-wsrep-log MariaDB Server, wsrep_25.10.r4144

Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
```

3.此处也可以将haproxy 写入服务
```
cat > /etc/init.d/haproxyd <<OO
#!/bin/bash
#chkconfig: 35 35 -
. /etc/init.d/functions
BASE="/usr/local/haproxy"
 
PROG=$BASE/sbin/haproxy
PIDFILE=$BASE/var/run/haproxy.pid
CONFFILE=$BASE/conf/haproxy.conf
 
case "$1" in
start)
         #$PROG  -f $CONFFILE >/dev/null 2>&1
         $PROG  -f $CONFFILE
       [ $?  -eq 0 ] && {
        action  "haproxy start is  OK..."  /bin/true
 } || action  "haproxy start is error..." /bin/false
         ;;
status)
         if [ !  -f $PIDFILE ]; then
                   echo  "pid not found"
                   exit  1
         fi
         for  pid in $(cat $PIDFILE); do
                   kill  -0 $pid
                   RETVAL="$?"
                   if  [ ! "$RETVAL" = "0" ]; then
                            echo  "process $pid died"
                            exit  1
                   fi
         done
         echo  "process is running"
         ;;
restart)
         kill  $(cat $PIDFILE)
        [ $? -eq 0 ] && {
        action  "haproxy stop is  OK..."  /bin/true
 } || action  "haproxy stop is error..." /bin/false
         #$PROG  -f $CONFFILE -sf $(cat $PIDFILE) >/dev/null 2>&1
         $PROG  -f $CONFFILE 
        [ $?  -eq 0 ] && {
        action  "haproxy start is  OK..."  /bin/true
 } || action  "haproxy start is error..." /bin/false
        ;;
stop)
         kill  $(cat $PIDFILE)
[ $? -eq 0 ] && {
        action  "haproxy stop is  OK..."  /bin/true
 } || action  "haproxy stop is error..." /bin/false
         ;;
*)
         echo  "USAGE: $0 start|restart|status|stop"
         exit 1
         ;;
esac
OO

chkconfig -add haproxy
```
# 注：
如果报如下错：
service haproxyd does not support chkconfig
解决方法：
在/etc/init.d/haproxyd 中添加下面两句到 #!/bin/bash之后，添加如下内容：
# chkconfig: 2345 10 90
# description:haproxy
----其中2345是默认启动级别，级别有0-6共7个级别。
----等级0表示：表示关机
----等级1表示：单用户模式
----等级2表示：无网络连接的多用户命令行模式
----等级3表示：有网络连接的多用户命令行模式
----等级4表示：不可用
----等级5表示：带图形界面的多用户模式
----等级6表示：重新启动
----10是启动优先级，90是停机优先级，优先级范围是0-100，数字越大，优先级越低。
4.日志支持功能添加
# vim /etc/rsyslog.conf
在最下边增加
local0.* /usr/local/haproxy/logs/haproxy.log
# 重启日志服务
```
/etc/init.d/rsyslog restart
```
MariaDB [(none)]> 
此时已经连接上集群了
为了区别他每次查询是否分发了我将一台集群撤销
```
[root@zyz_dba_test02 ~]# cat /etc/my.cnf 

# The following options will be passed to all MySQL clients
[client]
#password       = your_password
port            = 3306
socket          = /tmp/mysql.sock


[mysqld]
port            = 3306
user=mariadb
socket          = /tmp/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
datadir=/data/db

log-bin=mysql-bin
max_connections=2000

binlog_format = ROW

server-id       = 2

innodb_autoinc_lock_mode = 2

#wsrep_provider = /usr/local/mysql/lib/libgalera_smm.so

#wsrep_cluster_name = "my_mariadb_cluster"

#wsrep_cluster_address="gcomm://10.21.3.106,10.21.3.108"
#wsrep_cluster_name='example_cluster'

#wsrep_node_name = "cluster_node2"

#wsrep_node_address = 10.21.3.107:4406
#wsrep_sst_auth=tt:123
#wsrep_node_name='node2'
#wsrep_sst_method=rsync
```


注释掉107的集群
重启107的数据库
```
[root@zyz_dba_test02 ~]# service  mysqld restart
Shutting down MySQL..
MariaDB [(none)]> drop database zyy;
MariaDB [(none)]> show databases;
+--------------------+
| Database           |
+--------------------+
| haha               |
| information_schema |
| mysql              |
| performance_schema |
| sbtest             |
| test               |
+--------------------+
6 rows in set (0.01 sec)
而108和106的数据库集群还在情况是这样的
MariaDB [(none)]> show databases;
+--------------------+
| Database           |
+--------------------+
| haha               |
| information_schema |
| mysql              |
| performance_schema |
| sbtest             |
| test               |
| zyz                |
+--------------------+
7 rows in set (0.00 sec)
```

登陆109 也就是haproxy 这台服务器
查看三次
```
[root@zyz_dba_test04 haproxy]# mysql -h 10.21.3.109  -uzuo -p123  --port 3399 -e "show databases;"
+--------------------+
| Database           |
+--------------------+
| haha               |
| information_schema |
| mysql              |
| performance_schema |
| sbtest             |
| test               |
| zyz                |
+--------------------+
[root@zyz_dba_test04 haproxy]# mysql -h 10.21.3.109  -uzuo -p123  --port 3399 -e "show databases;"
+--------------------+
| Database           |
+--------------------+
| haha               |
| information_schema |
| mysql              |
| performance_schema |
| sbtest             |
| test               |
| zyz                |
+--------------------+
[root@zyz_dba_test04 haproxy]# mysql -h 10.21.3.109  -uzuo -p123  --port 3399 -e "show databases;"
+--------------------+
| Database           |
+--------------------+
| haha               |
| information_schema |
| mysql              |
| performance_schema |
| sbtest             |
| test               |
+--------------------+
```
发现了最后一次查询里面没有zyz说明是轮询的查询的
4.压力测试比较
1.首先在集群中的某一台上压测
```
[root@zyz_dba_test03 ~]#  sysbench --test=/usr/share/doc/sysbench/tests/db/oltp.lua --oltp_tables_count=9 --oltp-table-size=10 --rand-init=on --num-threads=12  --oltp-read-only=off --report-interval=1 --rand-type=gaussian --max-time=3000 --max-requests=0 --mysql-host=10.21.3.108 --mysql-port=3306 --mysql-user=zuo --mysql-password=123 run
sysbench 0.5:  multi-threaded system evaluation benchmark

Running the test with following options:
Number of threads: 12
Report intermediate results every 1 second(s)
Initializing random number generator from timer.

Random number generator seed is 0 and will be ignored


Threads started!

[   1s] threads: 12, tps: 43.16, reads/s: 721.72, writes/s: 185.70, response time: 376.98ms (95%)
[   2s] threads: 12, tps: 52.09, reads/s: 721.26, writes/s: 201.35, response time: 375.85ms (95%)
[   3s] threads: 12, tps: 48.00, reads/s: 681.00, writes/s: 194.00, response time: 328.68ms (95%)
[   4s] threads: 12, tps: 49.00, reads/s: 694.00, writes/s: 198.00, response time: 304.16ms (95%)
[   5s] threads: 12, tps: 53.00, reads/s: 717.01, writes/s: 206.00, response time: 398.44ms (95%)
[   6s] threads: 12, tps: 50.00, reads/s: 714.00, writes/s: 198.00, response time: 348.65ms (95%)
[   7s] threads: 12, tps: 48.00, reads/s: 688.00, writes/s: 200.00, response time: 360.53ms (95%)
[   8s] threads: 12, tps: 35.00, reads/s: 503.00, writes/s: 141.00, response time: 563.18ms (95%)
[   9s] threads: 12, tps: 40.00, reads/s: 597.95, writes/s: 181.99, response time: 404.09ms (95%)
[  10s] threads: 12, tps: 44.93, reads/s: 600.01, writes/s: 158.74, response time: 501.13ms (95%)
[  11s] threads: 12, tps: 52.09, reads/s: 716.23, writes/s: 207.35, response time: 312.56ms (95%)
[  12s] threads: 12, tps: 47.00, reads/s: 694.00, writes/s: 207.00, response time: 380.26ms (95%)
[  13s] threads: 12, tps: 48.00, reads/s: 633.01, writes/s: 166.00, response time: 401.20ms (95%)
[  14s] threads: 12, tps: 43.00, reads/s: 639.99, writes/s: 185.00, response time: 309.77ms (95%)
[  15s] threads: 12, tps: 51.00, reads/s: 670.02, writes/s: 193.01, response time: 401.92ms (95%)
[  16s] threads: 12, tps: 51.00, reads/s: 717.00, writes/s: 204.00, response time: 286.23ms (95%)
```

2.在测haproxy  在测试的时候在3台集群上都查看了cpu 压力
```
[root@zyz_dba_test03 ~]#  sysbench --test=/usr/share/doc/sysbench/tests/db/oltp.lua --oltp_tables_count=9 --oltp-table-size=10 --rand-init=on --num-threads=12  --oltp-read-only=off --report-interval=1 --rand-type=gaussian --max-time=3000 --max-requests=0 --mysql-host=10.21.3.109 --mysql-port=3399 --mysql-user=zuo --mysql-password=123 run
sysbench 0.5:  multi-threaded system evaluation benchmark

Running the test with following options:
Number of threads: 12
Report intermediate results every 1 second(s)
Initializing random number generator from timer.

Random number generator seed is 0 and will be ignored


Threads started!

[   1s] threads: 12, tps: 108.05, reads/s: 1652.80, writes/s: 450.22, response time: 253.10ms (95%)
[   2s] threads: 12, tps: 126.00, reads/s: 1765.04, writes/s: 514.01, response time: 237.11ms (95%)
[   3s] threads: 12, tps: 92.00, reads/s: 1249.98, writes/s: 346.99, response time: 361.18ms (95%)
[   4s] threads: 12, tps: 146.00, reads/s: 2063.02, writes/s: 592.01, response time: 125.40ms (95%)
[   5s] threads: 12, tps: 135.98, reads/s: 1893.73, writes/s: 551.92, response time: 231.91ms (95%)
[   6s] threads: 12, tps: 105.01, reads/s: 1484.15, writes/s: 410.04, response time: 246.15ms (95%)
[   7s] threads: 12, tps: 132.00, reads/s: 1831.06, writes/s: 521.02, response time: 256.91ms (95%)
[   8s] threads: 12, tps: 133.00, reads/s: 1886.02, writes/s: 537.01, response time: 131.00ms (95%)
[   9s] threads: 12, tps: 89.99, reads/s: 1285.91, writes/s: 387.97, response time: 267.43ms (95%)
[  10s] threads: 12, tps: 116.01, reads/s: 1584.11, writes/s: 438.03, response time: 626.89ms (95%)
[  11s] threads: 12, tps: 139.00, reads/s: 1942.99, writes/s: 551.00, response time: 159.86ms (95%)
[  12s] threads: 12, tps: 115.00, reads/s: 1629.96, writes/s: 468.99, response time: 213.65ms (95%)
[  13s] threads: 12, tps: 140.00, reads/s: 1950.05, writes/s: 550.01, response time: 179.98ms (95%)
[  14s] threads: 12, tps: 122.99, reads/s: 1739.85, writes/s: 502.96, response time: 241.77ms (95%)
[  15s] threads: 12, tps: 126.01, reads/s: 1689.14, writes/s: 488.04, response time: 271.87ms (95%)
[  16s] threads: 12, tps: 148.00, reads/s: 2107.01, writes/s: 600.00, response time: 110.75ms (95%)
[  17s] threads: 12, tps: 139.00, reads/s: 1971.95, writes/s: 569.99, response time: 111.05ms (95%)
```

结论在haproxy上明显tps和读写能力都上升了利用到了集群

3.Keepalive
这里作用是给haproxy做一个高可用防止haproxy挂掉了 

然后再在给每一台mariadb 都做mycat  那么就是负载均衡加上路由了
再给每一台的mariadb 做成的mycat做成高可用 
至此基本完毕


二.MySQL+galera+haproxy+keepalived
网站http://galeracluster.com/
1.介绍
1.GALERA集群的好处 
Galera集群为MySQL生态系统提供了一个高可用性有明显的改善。 实现高可用性的各种方法通常只提供了一些特性可以通过Galera集群,使选择高可用性解决方案的一种权衡。
通过Galera集群可用下列特性:
?	真正的多主机 读和写任何节点在任何时候。
?	同步复制 没有slave滞后,没有数据丢失在节点崩溃。
?	紧耦合的 所有节点拥有相同的状态。 不允许不同节点之间的数据。
?	多线程的slave 获得更好的性能。 对于任何工作负载。
?	没有vip的主从故障转移操作或使用。No Master-Slave Failover(失效备援) Operations or Use of VIP
?	热备份 故障转移期间没有停机时间(因为没有故障转移)。
?	自动节点配置 不需要手动备份数据库并将其复制到新节点。
?	支持InnoDB。
?	透明的应用程序 不需要(或很少)更改)应用程序。
?	不需要读和写的分离。
结果是一个高可用性解决方案,既健壮的数据完整性和高性能与即时故障转移。


2.安装mysql集群
IP环境等与上面相同关闭selinux iptables 
或者
iptables --append INPUT --protocol tcp \      --source 10.21.3.106 --jump ACCEPT
# iptables --apend INPUT --protocol tcp \      --source 10.21.3.107 --jump ACCEPT
# iptables --append INPUT --protocol tcp \      --source 10.21.3.108--jump ACCEPT
Service  save iptables 
# iptables-save > /etc/sysconfig/iptables
```
106 107 108上执行以下安装操作配置文件为106服务器的106 为node1
yum -y install nmap
yum -y install perl-DBD-MySQL perl-Time-HiRes nc  install boost-program-options
 
rpm -ivh   percona-xtrabackup-2.1.8-733.rhel6.x86_64*.rpm
rpm -ivh galera-3-25.3.10-2.el6.x86_64.rpm 
tar -xf mysql-wsrep-5.6.23-25.10-linux-x86_64.tar.gz -C /usr/local/services/
cd /usr/local/services/
 mv mysql-wsrep-5.6.23-25.10-linux-x86_64/ mysql/
chown mysql. /data/db -R
[root@zyz_dba_test01 services]# cd mysql/
[root@zyz_dba_test01 mysql]#  ./scripts/mysql_install_db  --datadir=/data/db/ --basedir=/usr/local/services/mysql/ --user=mysql
yum remove postfix
 yum install postfix

cat  > /usr/local/mysql/my1.cnf  <<OO
[client]
port            = 3306
socket          = /tmp/mysql.sock
[mysqld]
port            = 3306
basedir=//usr/local/services/mysql
datadir=/data/db
user=mysql
socket          = /tmp/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
log-bin=mysql-bin
binlog_format = ROW
server-id       = 1
innodb_autoinc_lock_mode = 2
wsrep_provider = /usr/local/services/mysql/lib/libgalera_smm.so
wsrep_cluster_name = "my_mariadb_cluster"
wsrep_cluster_address="gcomm://"
wsrep_cluster_name='example_cluster'
wsrep_node_name = "cluster_node1"

wsrep_node_address = 10.21.3.106:4406
wsrep_sst_auth=tt:123
wsrep_node_name='node1'
wsrep_sst_method=rsync

[mysqldump]
quick
max_allowed_packet = 16M
[mysql]
no-auto-rehash
[myisamchk]
key_buffer_size = 8M
sort_buffer_size = 8M
[mysqlhotcopy]
interactive-timeout
OO


ln -sf /usr/lib64/galera-3/libgalera_smm.so /usr/local/services/mysql/lib/
ln -sf /usr/lib64/libssl.so.10 /usr/lib64/libssl.so.6
ln -sf /usr/lib64/libcrypto.so.10 /usr/lib64/libcrypto.so.6

 ./mysqld --defaults-file=/usr/local/services/mysql/my1.cnf --wsrep-new-cluster &
```
node2 node3 与上面noede1是配置文件不同此外直接service mysqld start方式启动 其他基本一致
当然这里我把位置换到/usr/local/services/mysql 其目的就是看看有的人换个位置是不是就不行了
 
至此安装完毕

3.其他两台大约的安装方法
```
tar -xf mysql-wsrep-5.6.23-25.10-linux-x86_64.tar.gz -C /usr/local/
yum -y install nmap
yum -y install perl-DBD-MySQL perl-Time-HiRes nc  install boost-program-options
cd /usr/local/mysql
cd scripts/
./mysql_install_db --datadir=/data/db/ --basedir=/usr/local/mysql/ --user=mysql
 rpm -ivh galera-3-25.3.10-2.el6.x86_64.rpm 
ln -sf /usr/lib64/galera-3/libgalera_smm.so /usr/local/mysql/lib/
ln -sf /usr/lib64/libssl.so.10 /usr/lib64/libssl.so.6
  ln -sf /usr/lib64/libcrypto.so.10 /usr/lib64/libcrypto.so.6

107(node2)上
cat > /etc/my.cnf << FF
# The following options will be passed to all MySQL clients
[client]
#password       = your_password
port            = 3306
socket          = /tmp/mysql.sock
[mysqld]
port            = 3306
user=mysql
datadir=/data/db
basedir=/usr/local/mysql
socket          = /tmp/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
datadir=/data/db
log-bin=mysql-bin
max_connections=2000
binlog_format = ROW
server-id       = 2
innodb_autoinc_lock_mode = 2
wsrep_provider = /usr/local/mysql/lib/libgalera_smm.so
wsrep_cluster_name = "my_mariadb_cluster"
wsrep_cluster_address="gcomm://10.21.3.106,10.21.3.108"
wsrep_cluster_name='example_cluster'
wsrep_node_name = "cluster_node2"
wsrep_node_address = 10.21.3.107:4406
wsrep_sst_auth=tt:123
wsrep_node_name='node2'
wsrep_sst_method=rsync

[mysqldump]
quick
max_allowed_packet = 16M
[mysql]
no-auto-rehash
# Remove the next comment character if you are not familiar with SQL
[myisamchk]
key_buffer_size = 8M
sort_buffer_size = 8M
[mysqlhotcopy]
interactive-timeout
FF


108（node3上）

cat > /etc/my.cnf << PP
# The follo#wing options will be passed to all MySQL clients
[client]
#password       = your_password
port            = 3306
socket          = /tmp/mysql.sock
[mysqld]
port            = 3306
user=mysql
datadir=/data/db
basedir=/usr/local/mysql
socket          = /tmp/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
datadir=/data/db
 max_connections=10000 
log-bin=mysql-bin
binlog_format = ROW
server-id       = 3
innodb_autoinc_lock_mode = 2
wsrep_provider = /usr/local/mysql/lib/libgalera_smm.so
wsrep_cluster_name = "my_mariadb_cluster"
wsrep_cluster_address="gcomm://10.21.3.106,10.21.3.107"
wsrep_cluster_name='example_cluster'
wsrep_node_name = "cluster_node3"
wsrep_node_address = 10.21.3.108:4406
wsrep_sst_auth=tt:123
wsrep_node_name='node3'
wsrep_sst_method=rsync
[mysqldump]
quick
max_allowed_packet = 16M
[mysql]
no-auto-rehash
[myisamchk]
key_buffer_size = 8M
sort_buffer_size = 8M
[mysqlhotcopy]
interactive-timeout
PP
```

都service mysqld start 结束
4.安装haproxy
按照上面的haproxy安装就行了
5.sysbench 压测
```
[root@zyz_dba_test03 ~]# sysbench --test=/usr/share/doc/sysbench/tests/db/oltp.lua --oltp_tables_count=9 --oltp-table-size=100000 --rand-init=on --num-threads=10  --oltp-read-only=off --report-interval=1 --rand-type=gaussian --max-time=60 --max-requests=0 --mysql-host=10.21.3.109 --mysql-port=3399 --mysql-user=zuo --mysql-password=123 run
sysbench 0.5:  multi-threaded system evaluation benchmark

Running the test with following options:
Number of threads: 10
Report intermediate results every 1 second(s)
Initializing random number generator from timer.

Random number generator seed is 0 and will be ignored


Threads started!

[   1s] threads: 10, tps: 0.00, reads/s: 140.15, writes/s: 24.03, response time: 0.00ms (95%)
[   2s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[   3s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[   4s] threads: 10, tps: 41.00, reads/s: 684.03, writes/s: 199.01, response time: 3345.15ms (95%)
[   5s] threads: 10, tps: 101.01, reads/s: 1525.09, writes/s: 414.02, response time: 425.57ms (95%)
[   6s] threads: 10, tps: 118.99, reads/s: 1866.91, writes/s: 510.98, response time: 224.47ms (95%)
[   7s] threads: 10, tps: 101.00, reads/s: 1605.98, writes/s: 444.99, response time: 209.35ms (95%)
[   8s] threads: 10, tps: 124.99, reads/s: 1941.91, writes/s: 520.97, response time: 246.74ms (95%)
[   9s] threads: 10, tps: 91.00, reads/s: 1503.08, writes/s: 422.02, response time: 328.29ms (95%)
[  10s] threads: 10, tps: 128.00, reads/s: 2021.01, writes/s: 539.00, response time: 133.14ms (95%)
[  11s] threads: 10, tps: 57.00, reads/s: 849.94, writes/s: 252.98, response time: 446.58ms (95%)
[  12s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  13s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  14s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  15s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  16s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  17s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  18s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  19s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  20s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  21s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  22s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
[  23s] threads: 10, tps: 0.00, reads/s: 0.00, writes/s: 0.00, response time: 0.00ms (95%)
```
看到没 就这个测试结果谁还敢用mysql 装mariadb集群插件直接宣告失败

三.Percona XtraDB Cluster
1.安装配置
环境还是上面的环境
106 node1 
107 node2
108 node3
109 haproxy 
这次用3307端口
自行下载percona集群软件
```
useradd mysql
mkdir /opt/data
chown mysql. /opt/data
chown 777 /opt
chown mysql. /opt
 tar -xf /opt/Percona-XtraDB-Cluster-5.6.24-rel72.2-25.11..Linux.x86_64.tar.gz 
mv /opt/Percona-XtraDB-Cluster-5.6.24-rel72.2-25.11..Linux.x86_64 /opt/percona
rpm -ivh Percona-XtraDB-Cluster-galera-3-3.11-1.rhel6.x86_64.rpm


[root@zyz_dba_test03 opt]# cat /etc/my.cnf 

# The follo#wing options will be passed to all MySQL clients
[client]
#password       = your_password
port            = 3307
socket          = /opt/mysql.sock


[mysqld]
port            = 3307
user=mysql
datadir=/opt/data
basedir=/opt/percona
socket          = /opt/mysql.sock
skip-external-locking
key_buffer_size = 16K
max_allowed_packet = 1M
table_open_cache = 4
sort_buffer_size = 64K
read_buffer_size = 256K
read_rnd_buffer_size = 256K
net_buffer_length = 2K
thread_stack = 240K
 max_connections=10000 
log-bin=mysql-bin
binlog_format = ROW
server-id       = 3
innodb_autoinc_lock_mode = 2
wsrep_provider = /usr/lib64/galera3/libgalera_smm.so
wsrep_cluster_name = "my_mariadb_cluster"
wsrep_cluster_address="gcomm://10.21.3.106,10.21.3.107"
wsrep_cluster_name='example_cluster'
wsrep_node_name = "cluster_node3"
wsrep_node_address = 10.21.3.108:4406 #写自己的IP
wsrep_sst_auth=tt:123
wsrep_node_name='node3'
wsrep_sst_method=rsync
[mysqldump]
quick
max_allowed_packet = 16M
[mysql]
no-auto-rehash
[myisamchk]
key_buffer_size = 8M
sort_buffer_size = 8M
[mysqlhotcopy]
interactive-timeout
```
三台基本都是这样配置的
```
106 
wsrep_cluster_address="gcomm://"
```
这个不同而已未标红色的都一样
按照node1到node3的顺序启动
```
Node1:
/opt/percona/bin/mysqld --defaults-file=/opt/my.cnf --wsrep-new-cluster
Node2 Node3:
cp /opt/percona/support-files/mysql.server  /etc/init.d/percona
vi /etc/init.d/percona
找到basedir和datadir改称如下
basedir=/opt/percona
datadir=/opt/data
```
2.配haproxy
与上面的配制方式一致
3.压力测试
之中我混杂过只要里面的成员有mysql  tps就有0的出现
```
 


    response time:
         min:                                226.48ms
         avg:                               1265.01ms
         max:                               9628.06ms
         approx.  95 percentile:            3422.12ms

Threads fairness:
    events (avg/stddev):           47.7656/7.15
    execution time (avg/stddev):   60.4241/0.28
RECORD LOCKS space id 26 page no 7 n bits 144 index `GEN_CLUST_INDEX` of table `sbtest`.`sbtest9` trx id 166278 lock_mode X locks rec but not gap

```
在106那端出现大量的这个X锁信息
全局都用percona cluster成员的压测
负载大约在1.5左右的情况下
```
[root@zyz_dba_test03 ~]# sysbench --test=/usr/share/doc/sysbench/tests/db/oltp.lua --oltp_tables_count=9 --oltp-table-size=10 --rand-init=on --num-threads=12  --oltp-read-only=off --report-interval=1 --rand-type=gaussian --max-time=3000 --max-requests=0 --mysql-host=10.21.3.109 --mysql-port=3399 --mysql-user=zuo --mysql-password=123 run
sysbench 0.5:  multi-threaded system evaluation benchmark

Running the test with following options:
Number of threads: 12
Report intermediate results every 1 second(s)
Initializing random number generator from timer.

Random number generator seed is 0 and will be ignored


Threads started!

[   1s] threads: 12, tps: 77.96, reads/s: 1175.45, writes/s: 315.88, response time: 499.33ms (95%)
[   2s] threads: 12, tps: 128.00, reads/s: 1843.01, writes/s: 522.00, response time: 123.21ms (95%)
[   3s] threads: 12, tps: 114.00, reads/s: 1584.98, writes/s: 455.99, response time: 235.69ms (95%)
[   4s] threads: 12, tps: 127.85, reads/s: 1796.83, writes/s: 510.38, response time: 117.66ms (95%)
[   5s] threads: 12, tps: 128.16, reads/s: 1755.14, writes/s: 511.62, response time: 136.57ms (95%)
[   6s] threads: 12, tps: 131.00, reads/s: 1870.00, writes/s: 526.00, response time: 114.94ms (95%)
[   7s] threads: 12, tps: 123.00, reads/s: 1722.00, writes/s: 486.00, response time: 178.42ms (95%)
[   8s] threads: 12, tps: 121.00, reads/s: 1687.01, writes/s: 479.00, response time: 159.86ms (95%)
[   9s] threads: 12, tps: 132.00, reads/s: 1860.99, writes/s: 526.00, response time: 114.49ms (95%)
[  10s] threads: 12, tps: 127.00, reads/s: 1764.00, writes/s: 512.00, response time: 139.46ms (95%)
[  11s] threads: 12, tps: 104.00, reads/s: 1458.99, writes/s: 432.00, response time: 132.42ms (95%)
[  12s] threads: 12, tps: 97.00, reads/s: 1399.99, writes/s: 390.00, response time: 328.78ms (95%)
[  13s] threads: 12, tps: 107.00, reads/s: 1459.03, writes/s: 420.01, response time: 143.78ms (95%)
[  14s] threads: 12, tps: 102.80, reads/s: 1417.27, writes/s: 403.22, response time: 134.78ms (95%)
[  15s] threads: 12, tps: 93.17, reads/s: 1345.47, writes/s: 373.69, response time: 387.39ms (95%)
[  16s] threads: 12, tps: 118.01, reads/s: 1597.14, writes/s: 471.04, response time: 208.28ms (95%)
[  17s] threads: 12, tps: 128.00, reads/s: 1820.00, writes/s: 508.00, response time: 110.98ms (95%)
[  18s] threads: 12, tps: 121.00, reads/s: 1716.99, writes/s: 486.00, response time: 147.89ms (95%)
[  19s] threads: 12, tps: 109.00, reads/s: 1514.02, writes/s: 438.00, response time: 159.00ms (95%)
[  20s] threads: 12, tps: 127.00, reads/s: 1752.99, writes/s: 505.00, response time: 133.38ms (95%)
[  21s] threads: 12, tps: 126.00, reads/s: 1787.01, writes/s: 514.00, response time: 120.00ms (95%)
[  22s] threads: 12, tps: 132.00, reads/s: 1843.00, writes/s: 521.00, response time: 112.32ms (95%)
[  23s] threads: 12, tps: 126.00, reads/s: 1773.98, writes/s: 509.00, response time: 120.40ms (95%)
[  24s] threads: 12, tps: 126.00, reads/s: 1776.00, writes/s: 504.00, response time: 120.00ms (95%)
[  25s] threads: 12, tps: 71.00, reads/s: 999.94, writes/s: 299.98, response time: 129.95ms (95%)
[  26s] threads: 12, tps: 85.00, reads/s: 1190.03, writes/s: 327.01, response time: 907.28ms (95%)
[  27s] threads: 12, tps: 110.99, reads/s: 1540.87, writes/s: 455.96, response time: 206.11ms (95%)
[  28s] threads: 12, tps: 127.01, reads/s: 1740.19, writes/s: 487.05, response time: 140.26ms (95%)
[  29s] threads: 12, tps: 126.00, reads/s: 1789.04, writes/s: 507.01, response time: 113.50ms (95%)
[  30s] threads: 12, tps: 129.00, reads/s: 1789.00, writes/s: 519.00, response time: 108.88ms (95%)
[  31s] threads: 12, tps: 113.00, reads/s: 1603.96, writes/s: 453.99, response time: 172.02ms (95%)
[  32s] threads: 12, tps: 129.00, reads/s: 1804.02, writes/s: 510.01, response time: 112.02ms (95%)
[  33s] threads: 12, tps: 124.00, reads/s: 1729.00, writes/s: 504.00, response time: 127.48ms (95%)
[  34s] threads: 12, tps: 93.00, reads/s: 1335.96, writes/s: 383.99, response time: 195.59ms (95%)
[  35s] threads: 12, tps: 89.00, reads/s: 1192.96, writes/s: 331.99, response time: 543.15ms (95%)
[  36s] threads: 12, tps: 110.01, reads/s: 1555.11, writes/s: 454.03, response time: 152.34ms (95%)
[  37s] threads: 12, tps: 102.99, reads/s: 1440.91, writes/s: 411.97, response time: 177.51ms (95%)
[  38s] threads: 12, tps: 126.00, reads/s: 1767.07, writes/s: 496.02, response time: 127.75ms (95%)
[  39s] threads: 12, tps: 123.99, reads/s: 1764.83, writes/s: 501.95, response time: 119.79ms (95%)
[  40s] threads: 12, tps: 99.01, reads/s: 1388.17, writes/s: 406.05, response time: 331.84ms (95%)
[  41s] threads: 12, tps: 78.00, reads/s: 1092.00, writes/s: 312.00, response time: 502.63ms (95%)
[  42s] threads: 12, tps: 88.00, reads/s: 1241.99, writes/s: 351.00, response time: 408.10ms (95%)
[  43s] threads: 12, tps: 76.00, reads/s: 1031.00, writes/s: 298.00, response time: 483.16ms (95%)
[  44s] threads: 12, tps: 123.99, reads/s: 1741.93, writes/s: 494.98, response time: 118.08ms (95%)
[  45s] threads: 12, tps: 94.00, reads/s: 1287.03, writes/s: 362.01, response time: 385.42ms (95%)
[  46s] threads: 12, tps: 111.00, reads/s: 1560.04, writes/s: 451.01, response time: 153.16ms (95%)
[  47s] threads: 12, tps: 128.00, reads/s: 1800.99, writes/s: 508.00, response time: 118.79ms (95%)
[  48s] threads: 12, tps: 125.73, reads/s: 1780.13, writes/s: 503.90, response time: 119.93ms (95%)
[  49s] threads: 12, tps: 123.27, reads/s: 1721.74, writes/s: 496.08, response time: 126.08ms (95%)
[  50s] threads: 12, tps: 124.99, reads/s: 1739.92, writes/s: 501.98, response time: 119.43ms (95%)
[  51s] threads: 12, tps: 120.00, reads/s: 1676.05, writes/s: 485.01, response time: 133.10ms (95%)
[  52s] threads: 12, tps: 122.00, reads/s: 1738.03, writes/s: 489.01, response time: 132.34ms (95%)
[  53s] threads: 12, tps: 122.00, reads/s: 1659.00, writes/s: 471.00, response time: 143.61ms (95%)
[  54s] threads: 12, tps: 120.00, reads/s: 1685.00, writes/s: 487.00, response time: 138.75ms (95%)
```
超多线程压测 负载已经飙升到18啦的情况下
```
[root@zyz_dba_test03 ~]# sysbench --test=/usr/share/doc/sysbench/tests/db/oltp.lua --oltp_tables_count=9 --oltp-table-size=10 --rand-init=on --num-threads=64  --oltp-read-only=off --report-interval=1 --rand-type=gaussian --max-time=3000 --max-requests=0 --mysql-host=10.21.3.109 --mysql-port=3399 --mysql-user=zuo --mysql-password=123 run
sysbench 0.5:  multi-threaded system evaluation benchmark

Running the test with following options:
Number of threads: 64
Report intermediate results every 1 second(s)
Initializing random number generator from timer.

Random number generator seed is 0 and will be ignored


Threads started!

[   1s] threads: 64, tps: 84.58, reads/s: 1846.37, writes/s: 365.15, response time: 826.38ms (95%)
[   2s] threads: 64, tps: 122.04, reads/s: 1594.50, writes/s: 474.15, response time: 765.42ms (95%)
[   3s] threads: 64, tps: 124.00, reads/s: 1841.01, writes/s: 515.00, response time: 627.45ms (95%)
[   4s] threads: 64, tps: 135.00, reads/s: 1770.00, writes/s: 525.00, response time: 661.59ms (95%)
[   5s] threads: 64, tps: 125.00, reads/s: 1806.99, writes/s: 508.00, response time: 635.77ms (95%)
[   6s] threads: 64, tps: 131.99, reads/s: 1803.91, writes/s: 520.97, response time: 584.48ms (95%)
[   7s] threads: 64, tps: 106.00, reads/s: 1632.03, writes/s: 465.01, response time: 972.81ms (95%)
[   8s] threads: 64, tps: 26.00, reads/s: 585.97, writes/s: 149.99, response time: 1192.42ms (95%)
[   9s] threads: 64, tps: 130.00, reads/s: 1585.96, writes/s: 439.99, response time: 1562.04ms (95%)
[  10s] threads: 64, tps: 117.01, reads/s: 1715.17, writes/s: 507.05, response time: 671.36ms (95%)
[  11s] threads: 64, tps: 92.00, reads/s: 1141.00, writes/s: 314.00, response time: 1285.08ms (95%)
[  12s] threads: 64, tps: 126.00, reads/s: 1818.99, writes/s: 523.00, response time: 590.99ms (95%)
[  13s] threads: 64, tps: 125.00, reads/s: 1738.01, writes/s: 495.00, response time: 690.73ms (95%)
[  14s] threads: 64, tps: 131.00, reads/s: 1818.97, writes/s: 526.99, response time: 624.26ms (95%)
```

看看负载在1 也就是12线程的时候和负载在15的时候的tps基本上差不多
而上面的mariadb 是mysql5.5版本相对应的在负载十分高的情况下 tps 就会有所下降

因此在mariadb 还没有5.6版本出现的情况下 tps在负载不高的情况下比percona牛逼
但是负载高的情况下差与percona5.6.24版本
因此生产线哥哥选择了percona 集群

四．Keepalived
1.简单介绍
Keepalived是软件高可用可以做到自动检测接替，使用一个vip在keepalived安装机器上贴合当一台keepalived服务挂掉后服务中vip将会漂移到priority最大的服务组中  

2.安装配置
````
echo 'net.ipv4.ip_nonlocal_bind = 1'>>/etc/sysctl.conf
cd ~
wget http://www.keepalived.org/software/keepalived-1.2.9.tar.gz
tar  -xf keepalived-1.2.9.tar.gz 
mkdir /usr/local/keepalived 
cd ~/ keepalived-1.2.9
./configure  --prefix=/usr/local/keepalived/
make && make install
cp /usr/local/keepalived/sbin/keepalived  /usr/sbin/ 
cp /usr/local/keepalived/etc/sysconfig/keepalived  /etc/sysconfig
cp /usr/local/keepalived/etc/rc.d/init.d/keepalived  /etc/init.d/
chmod +x /etc/init.d/keepalived
mkdir /etc/keepalived
cat /etc/keepalived/check_haproxy.sh  
#!/bin/bash
A=`ps -C haproxy --no-header |wc -l`
if [ $A -eq 0 ];then
/etc/init.d/haproxyd restart
echo "Start haproxy" &> /dev/null
sleep 3
if [ `ps -C haproxy --no-header |wc -l` -eq 0 ];then
/etc/init.d/keepalived stop
echo "Stop keepalived" &> /dev/null
fi
fi

chmod +x /etc/keepalived/check_haproxy.sh

cat > /etc/keepalived/keepalived.conf << GG
global_defs {  
    notification_email {  
    zuoyuezong@163.com     
    }  
    notification_email_from zuoyuezong@163.com  
    smtp_server smtp.163.com   # 邮件服务器地址  
    smtp_connect_timeout 30     # 连接超时时间  
    router_id LVS_Master  
}  
vrrp_script chk_http_port {  
    script  "/etc/keepalived/check_haproxy.sh"    # haproxy运行检测脚本[haproxy宕掉重启haproxy服务]  
    interval        5                     # 脚本执行间隔  
    weight         -5                    # 执行脚本后优先级变更：5表示优先级+5;-5则表示优先级-5  
}  
vrrp_instance VI_A {  
    state       BACKUP                    # 主上此值为MASTER,从上为BACKUP  
    interface   eth0  
    virtual_router_id 50            # 此值主从必须一致  
    priority 80                    # 此值MASTER上比BACKUP 大 
    advert_int 1  
    authentication {                  # authentication两个参数值,主从也必须一致  
        auth_type PASS  
        auth_pass yiban
    }  
track_script {  
    chk_http_port  
    }  
    virtual_ipaddress {  
        10.21.82.41              # haproxy提供的虚拟IP地址  
    }  
}
GG


MASTER上keepalived配置
cat keepalived.conf
global_defs {  
    notification_email {  
 zuoyuezong@163.com
    }  
    notification_email_from zuoyuezong@163.com
    smtp_server smtp.163.com   # 邮件服务器地址  
    smtp_connect_timeout 30     # 连接超时时间  
    router_id LVS_Master  
}  
vrrp_script chk_http_port {  
    script  "/etc/keepalived/check_haproxy.sh"    # haproxy运行检测脚本[haproxy宕掉重启haproxy服务]  
    interval        5                     # 脚本执行间隔  
    weight         -5                    # 执行脚本后优先级变更：5表示优先级+5;-5则表示优先级-5  
}  
vrrp_instance VI_A {  
    state MASTER                    # 主上此值为MASTER,从上为BACKUP  
    interface eth0  
    virtual_router_id 50            # 此值主从必须一致  
    priority 100  
    advert_int 1  
    authentication {                  # authentication两个参数值,主从也必须一致  
        auth_type PASS  
    auth_pass yiban  
    }  
track_script {  
    chk_http_port  
    }  
    virtual_ipaddress {  
          10.21.82.41      
    }  
}  
````
